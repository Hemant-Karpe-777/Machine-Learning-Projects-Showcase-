# ğŸ”— Loan Approval Prediction using Stacking Ensemble & SHAP

This project demonstrates a powerful machine learning pipeline for classifying loan approvals using a **stacking ensemble** of Random Forest, XGBoost, and LightGBM, combined with **SHAP** for interpretability.

---

## ğŸ“Œ Problem Statement

Predict whether a loan will be approved based on customer and financial details, and **explain model decisions using SHAP** to ensure interpretability.

---

## ğŸ§  Techniques Used

| Component       | Description                                      |
|----------------|--------------------------------------------------|
| Data Cleaning   | Drop NAs, handle missing Loan_ID                |
| Preprocessing   | One-Hot Encoding of categorical features        |
| Models Used     | RandomForest, XGBoost, LightGBM                 |
| Meta Model      | Logistic Regression (for stacking)              |
| Evaluation      | Accuracy, F1-score, Classification Report       |
| Explainability  | SHAP summary & force plots for XGB & LGBM       |

---

## ğŸ“ Project Structure

```
StackedModel_Loan/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ loan_data.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ rf_model.pkl
â”‚   â”œâ”€â”€ xgb_model.pkl
â”‚   â”œâ”€â”€ lgb_model.pkl
â”‚   â””â”€â”€ stacked_meta_model.pkl
â”œâ”€â”€ shap/
â”‚   â”œâ”€â”€ shap_summary_xgb.png
â”‚   â”œâ”€â”€ shap_summary_lgbm.png
â”‚   â””â”€â”€ shap_force_individual.html
â”œâ”€â”€ images/
â”‚   â””â”€â”€ stacking_architecture.png
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Day4_Stacking_Intro.ipynb
â”‚   â””â”€â”€ Day5_Stacking_Train_Meta.ipynb
â”‚   â””â”€â”€ Day6_SHAP_Interpretation.ipynb
â”œâ”€â”€ README.md
```

---

## ğŸ”— Dataset

- **Source:** [Kaggle - Loan Approval Classification Data](https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data?select=loan_data.csv)
- Target variable: `Loan_Status`

---

## ğŸ”„ Workflow

### 1. Data Preprocessing
- Removed `Loan_ID`
- Encoded categorical columns using `pd.get_dummies()`
- Train/Test split using `stratify=y`

### 2. Base Learners
- Trained using 5-fold StratifiedKFold
- Used `predict_proba` for out-of-fold predictions
- Base models: `RandomForest`, `XGBoost`, `LightGBM`

### 3. Meta Learner
- Trained `LogisticRegression` on Level-1 predictions
- Combined all base predictions into a final stacked prediction

### 4. Model Evaluation

| Model     | Accuracy |
|-----------|----------|
| RF        | 0.9281    |
| XGBoost   | 0.9336    |
| LightGBM  | 0.9331    |
| **Stacked** | **0.9352** âœ… |

---

## ğŸ” SHAP Model Explanation

### Global Feature Importance
- SHAP summary plot shows top features across all predictions
- Used for XGB and LGBM separately

### Local Interpretation
- SHAP force plot shows why a specific prediction was made
- E.g., `ApplicantIncome`, `Credit_History`, and `LoanAmount` were top drivers

---

## ğŸ“Š Visuals

- `stacking_architecture.png` - Flow of base â†’ meta model
- `shap_summary_xgb.png` - Global SHAP plot for XGB
- `shap_force_individual.html` - Interactive force plot

---

## ğŸš€ Key Learnings

- Stacking boosts performance by leveraging multiple learners  
- SHAP enables model transparency (important in banking/finance)  
- Logistic regression is a simple yet effective meta learner  
- Model saving using `joblib` for reproducibility

---

## ğŸ”® Future Improvements

- Hyperparameter tuning with Optuna  
- Add CatBoost and NeuralNet to the stack  
- Deploy as an API with FastAPI or Streamlit  
- Perform feature selection and correlation filtering

---

## ğŸ™Œ Author

**Hemant Karpe**   
ğŸŒ [GitHub Portfolio](https://github.com/your-username)
ğŸ“§ Email: hemant.777karpe@gmail.com  
ğŸ”— LinkedIn: [Hemant-karpe](https://www.linkedin.com/in/hemant-karpe)

---

## ğŸ Final Note

This project reflects a real-world pipeline with reproducibility, performance, and interpretability â€“ ideal for portfolio or production-grade systems.
