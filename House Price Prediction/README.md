# ğŸ¡ House Price Prediction - Machine Learning Project

This project predicts house sale prices using machine learning regression techniques. It includes full end-to-end steps â€” from data exploration and cleaning to model training, evaluation, and explainability using SHAP.

---

## ğŸ“Œ Problem Statement

Given various features of residential homes (location, area, quality, garage, etc.), predict the `SalePrice` of a house.  
This is a regression task using the **Kaggle: House Prices - Advanced Regression Techniques** dataset.

---

## ğŸ“ Project Structure

â”œâ”€â”€ data/ # Raw dataset from Kaggle
â”œâ”€â”€ cleaned_data/ # Cleaned CSV used for modeling
â”œâ”€â”€ notebooks/ # All experiment notebooks (Day1 to Day6)
â”œâ”€â”€ models/ # Saved model files (.pkl)
â”œâ”€â”€ explainability/ # SHAP plots and analysis
â”œâ”€â”€ images/ # EDA plots and visuals
â”œâ”€â”€ README.md # This file


---

## ğŸ”§ Tools Used

- Python, NumPy, Pandas
- Matplotlib, Seaborn
- Scikit-learn (LinearRegression, RandomForest)
- XGBoost
- SHAP (model interpretability)
- Joblib (for saving model)
- Git + GitHub (version control)

---

## ğŸ“Š EDA Snapshots

### ğŸ”¹ Target Distribution: SalePrice
![Target](images/hist_target.png)

### ğŸ”¹ GrLivArea vs SalePrice
![Area vs Price](images/scatter_area_price.png)

---

## ğŸ§¼ Data Cleaning Steps

- Dropped high-null columns (`Alley`, `PoolQC`, etc.)
- Imputed missing numerical values using median
- Imputed categorical variables using mode
- One-hot encoded nominal categorical features
- Reduced skewness in highly skewed numerical columns

---

## ğŸ¤– Models Trained

| Model | MAE | RMSE | RÂ² Score |
|-------|-----|------|----------|
| Linear Regression | âœ… | âœ… | âœ… |
| Random Forest Regressor | âœ… | âœ… | âœ… |
| XGBoost Regressor | âœ… | âœ… | âœ… |

âœ… Metrics were computed using scikit-learn's `mean_absolute_error`, `mean_squared_error`, and `r2_score`.

---

## ğŸ” Model Explainability

SHAP (SHapley Additive exPlanations) was used to interpret model predictions.

- `explainability/shap_beeswarm.png` â€“ Global feature importance
- `explainability/shap_waterfall.png` â€“ Single prediction breakdown

---

## ğŸ“¦ Final Deliverables

- Trained XGBoost model saved as `models/xgb_model.pkl`
- Complete notebook: `notebooks/Day1_to_Day5.ipynb`
- Plots and insights saved in `/images/` and `/explainability/`
- GitHub repository with all files

---

## ğŸ“ˆ Future Improvements

- Feature engineering: polynomial features, feature selection
- Hyperparameter tuning with Optuna or GridSearchCV
- Streamlit-based web app deployment
